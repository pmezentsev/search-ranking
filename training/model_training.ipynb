{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38fc77a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Jupyter Notebook demonstrates how to build a ranking model from scratch using the Kernelized Neural Ranking Model (KNRM). We will train the KNRM model on the Quora question pairs dataset, which contains pairs of questions from Quora and labels indicating whether the questions are duplicates of each other.\n",
    "\n",
    "Ranking models are crucial in search engines, recommender systems, and other applications where items need to be ordered based on their relevance to a particular query. While it is possible to use general-purpose models for ranking, specialized models, such as KNRM, are designed to better capture the complex patterns and relationships in ranking tasks.\n",
    "\n",
    "In this notebook, we will walk through the process of installing dependencies, loading and preparing data, building the model, and training it. Finally, we will show how to evaluate the model using the normalized discounted cumulative gain (NDCG) metric.\n",
    "\n",
    "## Why separate models for ranking\n",
    "\n",
    "It is important to use separate models for ranking because ranking tasks have unique characteristics and requirements that general-purpose models might not adequately address. Ranking models are specifically designed to learn and optimize for the ranking task, allowing them to perform better than general-purpose models in this domain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f603c",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe42cf9",
   "metadata": {},
   "source": [
    "To run this notebook, you will need to install the following dependencies using pip:\n",
    "\n",
    "- numpy==1.19.2\n",
    "- pandas\n",
    "- torch==1.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b20bd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T15:56:14.474766Z",
     "start_time": "2023-03-12T15:55:24.385476Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install numpy==1.19.2 pandas torch==1.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9aa5060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T16:16:09.901947Z",
     "start_time": "2023-03-19T16:16:09.236726Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# some local modules where the complex code parts are implemented\n",
    "import aux\n",
    "import dataset\n",
    "from knrm import KNRM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543473e",
   "metadata": {},
   "source": [
    "# Quora question pairs dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a16af",
   "metadata": {},
   "source": [
    "The Quora question pairs dataset contains over 400,000 pairs of questions from the Quora platform, with binary labels indicating whether the questions are duplicates of each other. This dataset is an excellent choice for training a search relevance model because it provides a large number of diverse question pairs and relevance labels.\n",
    "\n",
    "To train our relevance ranking model, we will generate a new dataset that includes relevance levels for each pair. We will categorize the pairs into three groups:\n",
    "\n",
    "- Positive pairs, representing highly relevant samples;\n",
    "- Negative pairs, representing samples with low relevance;\n",
    "- Auto-generated pairs between random questions in the dataset, representing non-relevant samples. \n",
    "\n",
    "and mix them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce118db1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T15:56:32.572398Z",
     "start_time": "2023-03-12T15:56:17.707984Z"
    }
   },
   "outputs": [],
   "source": [
    "! mkdir -p resources && \\\n",
    "  wget https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip -O - | tar xz -C resources QQP/train.tsv QQP/dev.tsv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b9a1af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T15:58:42.239189Z",
     "start_time": "2023-03-19T15:58:41.205142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 363846\n",
      "Test size: 40430\n",
      "Dataset sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_left</th>\n",
       "      <th>id_right</th>\n",
       "      <th>text_left</th>\n",
       "      <th>text_right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133273</td>\n",
       "      <td>213221</td>\n",
       "      <td>213222</td>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402555</td>\n",
       "      <td>536040</td>\n",
       "      <td>536041</td>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  id_left  id_right  \\\n",
       "0  133273   213221    213222   \n",
       "1  402555   536040    536041   \n",
       "\n",
       "                                           text_left  \\\n",
       "0  How is the life of a math student? Could you d...   \n",
       "1                How do I control my horny emotions?   \n",
       "\n",
       "                                          text_right  label  \n",
       "0  Which level of prepration is enough for the ex...      0  \n",
       "1                 How do you control your horniness?      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_dir = './resources/QQP/'\n",
    "\n",
    "col_names = ['id', 'id_left', 'id_right', 'text_left', 'text_right', 'label']\n",
    "train_df = pd.read_csv(f\"{quora_dir}/train.tsv\", sep='\\t', names=col_names, skiprows=1)\n",
    "print(\"Train size:\", len(train_df))\n",
    "test_df = pd.read_csv(f\"{quora_dir}/dev.tsv\", sep='\\t', names=col_names, skiprows=1)\n",
    "print(\"Test size:\", len(test_df))\n",
    "print('Dataset sample:')\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8774bc48",
   "metadata": {},
   "source": [
    "To process the text data in the Quora question pairs dataset, we will first build a vocabulary containing all unique words in the dataset. This vocabulary will be used to convert the text data into numerical representations that can be fed into our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9563bfbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T15:59:40.331970Z",
     "start_time": "2023-03-19T15:59:35.166536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary len: 82459\n",
      "10 first tokens:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PAD', 'OOV', 'the', 'what', 'is', 'a', 'i', 'to', 'in', 'how']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_list = aux.build_vocabulary(train_df)\n",
    "print(\"Vocabulary len:\", len(vocabulary_list))\n",
    "print(\"10 first tokens:\")\n",
    "vocabulary_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc3fa8",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56bbb69",
   "metadata": {},
   "source": [
    "In this example, we use the GloVe (Global Vectors for Word Representation) pre-trained embeddings. GloVe is an unsupervised learning algorithm that obtains vector representations for words. These embeddings capture semantic and syntactic similarities between words, which can be useful for our ranking model.\n",
    "\n",
    "We will download the GloVe pre-trained embeddings and create a matrix containing the embeddings for each word in our vocabulary. This matrix will be used as the initial weights for our model's embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9795f66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T16:07:44.428672Z",
     "start_time": "2023-03-12T15:58:58.344772Z"
    }
   },
   "outputs": [],
   "source": [
    "! mkdir -p resources && \\\n",
    "  wget http://nlp.stanford.edu/data/glove.6B.zip -O - | tar xz -C resources glove.6B.50d.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc32cfb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T16:07:54.249783Z",
     "start_time": "2023-03-12T16:07:49.439842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82461, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.11888963,  0.19735816, -0.19424166, ..., -0.04189473,\n",
       "        -0.17693753,  0.16185388],\n",
       "       [-0.05232376,  0.13807736,  0.14311486, ...,  0.01411284,\n",
       "        -0.18726017,  0.11294341],\n",
       "       ...,\n",
       "       [-0.20304   , -0.42267   , -0.89636   , ...,  1.0393    ,\n",
       "        -0.11743   , -0.60719   ],\n",
       "       [-0.10024228, -0.09543935, -0.15954408, ...,  0.05721237,\n",
       "        -0.18037817,  0.03473483],\n",
       "       [ 0.11142   ,  0.54765   ,  0.45764   , ..., -0.95053   ,\n",
       "        -0.66657   ,  0.72248   ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_path = './resources/glove.6B.50d.txt'\n",
    "embeddings_matrix = aux.create_word_embeddings(glove_path, vocabulary_list)\n",
    "print(embeddings_matrix.shape)\n",
    "embeddings_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e418c4",
   "metadata": {},
   "source": [
    "# KNRM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791ac65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T22:03:38.458198Z",
     "start_time": "2022-09-16T22:03:38.455957Z"
    }
   },
   "source": [
    "The Kernelized Neural Ranking Model (KNRM) is a neural ranking model specifically designed for ranking tasks. It learns to map textual inputs into a continuous relevance space, allowing it to rank items based on their relevance to a given query.\n",
    "\n",
    "![topology](https://raw.githubusercontent.com/AdeDZY/K-NRM/master/model_simplified-1.png)\n",
    "\n",
    "The KNRM model consists of an embedding layer, kernelized matching layer, and a fully connected output layer. In this notebook, we will create a KNRM model using the pre-trained GloVe embeddings and train it on the Quora question pairs dataset.\n",
    "\n",
    "Here is the paper where you can read some details about it: [End-to-End Neural Ad-hoc Ranking with Kernel Pooling](https://arxiv.org/pdf/1706.06613.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78a62d21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T16:08:00.479940Z",
     "start_time": "2023-03-12T16:07:57.563948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNRM embeddings is created\n",
      "KNRM kernels is created\n",
      "KNRM mlp is created\n"
     ]
    }
   ],
   "source": [
    "model = KNRM(embeddings_matrix,\n",
    "             freeze_embeddings=True,\n",
    "             out_layers=[10,5],\n",
    "             kernel_num=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976e85b",
   "metadata": {},
   "source": [
    "# Datasets preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1406f69",
   "metadata": {},
   "source": [
    "To efficiently train and evaluate our model, we will create PyTorch DataLoaders for the training and test datasets. DataLoaders are useful because they handle batching, shuffling, and loading of data in parallel, making it easier to work with large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce5ff7fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T16:08:25.350367Z",
     "start_time": "2023-03-12T16:08:18.381496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f8aa982be10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = dataset.make_train_dataloader(train_df, vocabulary_list)\n",
    "test_dataloader = dataset.make_test_dataloader(test_df, vocabulary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc8a41",
   "metadata": {},
   "source": [
    "# Model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37123dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T16:13:09.236328Z",
     "start_time": "2023-03-19T16:13:09.231613Z"
    }
   },
   "source": [
    "In this section, we will train our KNRM model using a training loop that iterates over the training dataset for multiple epochs. During each epoch, \n",
    "the model will be updated using the stochastic gradient descent (SGD) optimizer and the binary cross-entropy loss function. After each epoch, we will evaluate the model's performance on the test dataset using the NDCG metric.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ad0d9",
   "metadata": {},
   "source": [
    "## NDCG metric\n",
    "Normalized Discounted Cumulative Gain (NDCG) is a widely used metric in ranking tasks. It evaluates the quality of a ranking by considering the relevance of each item in the ranked list and its position. NDCG is particularly suitable for ranking tasks because it takes into account both the order and relevance of items in the ranked list, making it more informative than other metrics like precision or recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fe92018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T16:11:07.503240Z",
     "start_time": "2023-03-12T16:11:07.495027Z"
    }
   },
   "outputs": [],
   "source": [
    "from metrics import ndcg_k\n",
    "\n",
    "def evaluate(model: torch.nn.Module = None, data: torch.utils.data.DataLoader = None) -> float:\n",
    "    labels_and_groups = data.dataset.samples_list\n",
    "    labels_and_groups = pd.DataFrame(labels_and_groups, columns=['left_id', 'right_id', 'rel'])\n",
    "\n",
    "    pred = [model.predict(batch).detach().numpy() for batch, _ in data]\n",
    "    pred = np.concatenate(pred, axis=0)\n",
    "    labels_and_groups['pred'] = pred\n",
    "\n",
    "    ndcg_list = [ndcg_k(df.rel.values, df.pred.values) for _, df in labels_and_groups.groupby('left_id')]\n",
    "    mean_ndcg = float(np.mean(ndcg_list))\n",
    "    return mean_ndcg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0bc0e8",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d9071",
   "metadata": {},
   "source": [
    "The training loop iterates over the training dataset for a specified number of epochs. During each epoch, the model's parameters are updated using the optimizer and the loss function. After each epoch, we evaluate the model's performance on the test dataset using the NDCG metric to track its progress.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8b39778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T16:11:07.574600Z",
     "start_time": "2023-03-12T16:11:07.571826Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "learning_rate = 0.01\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d3514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T16:37:45.894649Z",
     "start_time": "2023-03-12T16:11:07.610407Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss = torch.nn.BCELoss()\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    model.train()\n",
    "    for batch_idx, (left_batch, right_batch, y_true) in enumerate(train_dataloader):\n",
    "#         left_batch = {k: v for k, v in left_batch.items()}\n",
    "#         right_batch = {k: v for k, v in right_batch.items()}\n",
    "        opt.zero_grad()\n",
    "        y_pred = model.forward(left_batch, right_batch)\n",
    "        query_loss = loss(y_pred, y_true)\n",
    "        query_loss.backward()\n",
    "        opt.step()\n",
    "    ndcg_score = evaluate(model, test_dataloader)\n",
    "    print(f\"Epoch {epoch}. Test ndcg: {ndcg_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a0513",
   "metadata": {},
   "source": [
    "# Final words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fd5b8",
   "metadata": {},
   "source": [
    "Once the model is trained, we can use it for various applications, such as predicting the relevance of items in a search engine or a recommender system. To use the trained model, we need to save its parameters and load them back when needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resources/mlp_weights01.pkl', 'wb') as f:\n",
    "    torch.save(model.mlp.state_dict(), f)\n",
    "\n",
    "with open('resources/embeddings.pkl', 'wb') as f:\n",
    "    torch.save(model.embeddings.state_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b660fe",
   "metadata": {},
   "source": [
    "To load the saved model parameters, we can use PyTorch's torch.load() function and the load_state_dict() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_knrm_path, 'rb') as f:\n",
    "    emb_dict = torch.load(f,  map_location=torch.device('cpu'))\n",
    "\n",
    "with open(mlp_path, 'rb') as f:\n",
    "    mlp_dict = torch.load(f,  map_location=torch.device('cpu'))\n",
    "\n",
    "knrm_model = KNRM(emb_dict, mlp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a460d",
   "metadata": {},
   "source": [
    "With the trained model, you can now use it for various ranking tasks by providing input data in the same format as the training data. The model will output relevance scores, which can be used to rank items based on their relevance to a given query.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51d66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}